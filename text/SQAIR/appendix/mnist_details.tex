\section{Details of the moving-\textsc{mnist} Experiments}
\label{app:mnist_details}

\subsection{\textsc{Sqair} and \textsc{air} Training Details}
    All models are trained by maximising the \gls{ELBO} $\loss[IWAE]$ (\Cref{eq:iwae}) with the \gls{RMSprop} optimizer \citep{Tieleman2012rms} with momentum equal to $0.9$. We use the learning rate of $10^{-5}$ and decrease it to $\frac{1}{3} \cdot 10^{-5}$ after 400k and to $10^{-6}$ after 1000k training iterations. Models are trained for the maximum of $2 \cdot 10^6$ training iterations; we apply early stopping in case of overfitting.
    \Gls{SQAIR} models are trained with a curriculum of sequences of increasing length: we start with three time-steps, and increase by one time-step every $10^5$ training steps until reaching the maximum length of 10.
    When training \gls{AIR}, we treated all time-steps of a sequence as independent, and we trained it on all data (sequences of length ten, split into ten independent sequences of length one). 

\subsection{\textsc{Sqair} and \textsc{air} Model Architectures}
    All models use glimpse size of $20 \times 20$ and \gls{ELU} \citep{Clevert2015elu} non-linearities for all layers except \glspl{RNN} and output layers.
    \textsc{mlp}-\gls{SQAIR} uses fully-connected layers for all networks. In both variants of \gls{SQAIR}, the $\RD$ and $\Rr$ \glspl{RNN} are the vanilla \glspl{RNN}. The propagation prior \gls{RNN} and the temporal \gls{RNN} $\RT$ use \gls{GRU}. \gls{AIR} follows the same architecture as \textsc{mlp}-\gls{SQAIR}. All fully-connected layers and \glspl{RNN} in \textsc{mlp}-\gls{SQAIR} and \gls{AIR} have 256 units; they have $2.9$M and $1.7$M trainable parameters, respectively.
    
    \textsc{Conv}-\gls{SQAIR} differs from the  \textsc{mlp} version in that it uses \glspl{CNN} for the glimpse and image encoders and a subpixel-\gls{CNN} \citep{Shi2016subpixel} for the glimpse decoder. All fully connected layers and \glspl{RNN} have 128 units. 
    The encoders share the \gls{CNN}, which is followed by a single fully-connected layer (different for each encoder).
    The \gls{CNN} has four convolutional layers with $[16,32,32,64]$ features maps and strides of $[2,2,1,1]$. The glimpse decoder is composed of two fully-connected layers with $[256, 800]$ hidden units, whose outputs are reshaped into $32$ features maps of size $5 \times 5$, followed by a subpixel-\gls{CNN} with three layers of $[32,64,64]$ feature maps and strides of $[1, 2, 2]$.
    All filters are of size $3 \times 3$.
    \textsc{Conv}-\gls{SQAIR} has $2.6$M trainable parameters.
    
    We have experimented with different sizes of fully-connected layers and \glspl{RNN}; we kept the size of all layers the same and altered it in increments of 32 units. Values greater than 256 for \textsc{mlp}-\gls{SQAIR} and 128 for \textsc{conv}-\gls{SQAIR} resulted in overfitting. Models with as few as 32 units per layer ($<0.9$M trainable parameters for \textsc{mlp}-\gls{SQAIR}) displayed the same qualitative behaviour as reported models, but showed lower quantitative performance.
    
    The output likelihood used in both \gls{SQAIR} and \gls{AIR} is Gaussian with a fixed standard deviation set to $0.3$, as used by \cite{Eslami2016}. We tried using a learnable scalar standard deviation, but decided not to report it due to unsable behaviour in the early stages of training. Typically, standard deviation would converge to a low value early in training, which leads to high penalties for reconstruction mistakes. In this regime, it is beneficial for the model to perform no inference steps ($z^\mathrm{pres}$ is always equal to zero), and the model never learns. Fixing standard deviation for the first $10$k iterations and then learning it solves this issue, but it introduces unnecessary complexity into the training procedure.

\subsection{\textsc{Vrnn} Implementation and Training Details} \label{apd:vrnn}

    Our \gls{VRNN} implementation is based on the implementation\footnote{\url{https://github.com/tensorflow/models/tree/master/research/fivo}} of \gls{FIVO} by \cite{Maddison2017filtering}.
    We use an \textsc{lstm} with hidden size $J$ for the deterministic backbone of the \gls{VRNN}.
    At time $t$, the \textsc{lstm} receives $\psi^{x}(\bx_{t-1})$ and $\psi^{z}(\bz_{t-1})$ as input and outputs $o_t$, where $\psi^{x}$ is a data feature extractor and $\psi^{z}$ is a latent feature extractor. 
    The output is mapped to the mean and standard deviation of the Gaussian prior $\p{\bzt}{\bx_{t-1}}{\theta}$ by an \gls{MLP}. 
    The likelihood $\p{\bxt}{\bzt,\bx_{t-1}}{\theta}$ is a Gaussian, with mean given by $\psi^{\mathrm{dec}}(\psi^{z}(\bzt), o_t)$ and standard deviation fixed to be $0.3$ as for \gls{SQAIR} and \gls{AIR}. 
    The inference network $\q{\bzt}{\bz_{t-1},\bxt}{\phi}$ is a Gaussian with mean and standard deviation given by the output of separate \gls{MLP}s with inputs $[o_t,\psi^{x}(\bxt)]$.
    
    All aforementioned \glspl{MLP} use the same number of hidden units $H$ and the same number of hidden layers $L$.
    % We refer to this model as \textsc{mlp}-\gls{VRNN}.
    The \textsc{conv}-\gls{VRNN} uses a \gls{CNN} for $\psi^{x}$ and a transposed \gls{CNN} for $\psi^{\mathrm{dec}}$. The \textsc{mlp}-\gls{VRNN} uses an \gls{MLP} with $H'$ hidden units  and $L'$ hidden layers for both. 
    \Gls{ELU} were used throughout as activations. The latent dimensionality was fixed to 165, which is the upper bound of the number of latent dimensions that can be used per time-step in \gls{SQAIR} or \gls{AIR}. Training was done by optimising the \gls{FIVO} bound, which is known to be tighter than the \gls{IWAE} bound for sequential latent variable models \citep{Maddison2017filtering}. We also verified that this was the case with our models on the moving-\textsc{mnist} data. We train with the \gls{RMSprop} optimizer with a learning rate of $10^{-5}$, momentum equal to $0.9$, and training until convergence of test \gls{FIVO} bound.
    
    For each of \textsc{mlp}-\gls{VRNN} and \textsc{conv}-\gls{VRNN}, we experimented with three architectures: small/medium/large. 
    We used $H$=$H'$=$J$=128/256/512 and $L$=$L'$=2/3/4 for \textsc{mlp}-\gls{VRNN}, giving number of parameters of $1.2$M/$2.1$M/$9.8$M. 
    For \textsc{conv}-\gls{VRNN}, the number of features maps we used was $[32,32,64,64]$, $[32,32,32,64,64,64]$ and $[32,32,32,64,64,64,64,64,64]$, with strides of $[2,2,2,2]$, $[1,2,1,2,1,2]$ and $[1,2,1,2,1,2,1,1,1]$, all with $3 \times 3$ filters, $H$=$J$=$128$/$256$/$512$ and $L$=1, giving number of parameters of $0.8$M/$2.6$M/$6.1$M. 
    The largest convolutional encoder architecture is very similar to that in \cite{Gulrajani2016pixelvae} applied to \gls{MNIST}.
    
    We have chosen the medium-sized models for comparison with \gls{SQAIR} due to overfitting encountered in larger models.

\begin{table}
    \centering
    \caption{Number of trainable parameters for the reported models.}
    \label{tab:num_params}
    \begin{tabular}{c|c|c|c|c|c}
         & \textsc{conv}-\gls{SQAIR} & \textsc{mlp}-\gls{SQAIR} & \textsc{mlp}-\gls{AIR} & \textsc{conv}-\gls{VRNN} & \textsc{mlp}-\gls{VRNN}\\
                         \hline
         number of parameters & $2.6$M & $2.9$M & $1.7$M & $2.6$M  & $2.1$M 
    \end{tabular}
\end{table}

\subsection{Addition Experiment}

We perform the addition experiment by feeding latent representations extracted from the considered models into a 19-way classifier, as there are 19 possible outputs (addition of two digits between 0 and 9). 
The classifier is implemented as an \gls{MLP} with two hidden layers with 256 \gls{ELU} units each and a softmax output. For \gls{AIR} and \gls{SQAIR}, we use concatenated $\bz^\mathrm{what}$ variables multiplied by the corresponding $z^\mathrm{pres}$ variables, while for \gls{VRNN} we use the whole 165-dimensional latent vector.
We train the model over $10^7$ training iterations with the \gls{ADAM} optimizer \citep{Kingma2015adam} with default parameters (in tensorflow).