@inproceedings{Eslami2016,
abstract = {We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.},
archivePrefix = {arXiv},
eprint = {1603.08575},
author = {Eslami, S. M. Ali and Heess, Nicolas and Weber, Theophane and Tassa, Yuval and Szepesvari, David and Kavukcuoglu, Koray and Hinton, G. E.},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
title = {{Attend, Infer, Repeat: Fast Scene Understanding with Generative Models}},
% url = {http://arxiv.org/abs/1603.08575},
year = {2016}
}
@inproceedings{Gael2009,
abstract = {We introduce a new probability distribution over a potentially infinite number of binary Markov chains which we call the Markov Indian buffet process. This process extends the IBP to allow temporal dependencies in the hidden variables. We use this stochastic process to build a nonparametric extension of the factorial hidden Markov model. After constructing an inference scheme which combines slice sampling and dynamic programming we demonstrate how the infinite factorial hidden Markov model can be used for blind source separation.},
author = {Gael, Jurgen Van and Teh, Yee Whye and Ghahramani, Zoubin},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/adam/Documents/Mendeley/Gael, Teh, Ghahramani - 2009 - The Infinite Factorial Hidden Markov Model.pdf:pdf},
isbn = {9781605609492},
pages = {1697--1704},
title = {{The Infinite Factorial Hidden Markov Model}},
url = {https://papers.Advances in Neural Information Processing Systems.cc/paper/3518-the-infinite-factorial-hidden-markov-model},
year = {2009}
}
@article{Graves2016,
abstract = {Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external readâ€“write memory.},
author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'{n}}ska, Agnieszka and Colmenarejo, Sergio G{\'{o}}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
issn = {0028-0836},
journal = {Nature},
month = oct,
number = {7626},
pages = {471--476},
publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
title = {{Hybrid computing using a neural network with dynamic external memory}},
url = {http://dx.doi.org/10.1038/nature20101 http://10.0.4.14/nature20101 http://www.nature.com/nature/journal/v538/n7626/abs/nature20101.html{\#}supplementary-information},
volume = {538},
year = {2016}
}
@inproceedings{Jaderberg2015,
abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02025v1},
author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
booktitle = {Advances in Neural Information Processing Systems},
doi = {10.1038/nbt.3343},
eprint = {1506.02025v1},
isbn = {9781627480031},
issn = {1087-0156},
pmid = {26571099},
title = {{Spatial Transformer Networks}},
year = {2015}
}
@inproceedings{kosiorek2017hierch,
abstract = {Class-agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models cannot be learned a priori. Inspired by how the human visual cortex employs spatial attention and separate "where" and "what" processing pathways to actively suppress irrelevant visual features, this work develops a hierarchical attentive recurrent model for single object tracking in videos. The first layer of attention discards the majority of background by selecting a region containing the object of interest, while the subsequent layers tune in on visual features particular to the tracked object. This framework is fully differentiable and can be trained in a purely data driven fashion by gradient methods. To improve training convergence, we augment the loss function with terms for a number of auxiliary tasks relevant for tracking. Evaluation of the proposed model is performed on two datasets of increasing difficulty: pedestrian tracking on the KTH activity recognition dataset and the KITTI object tracking dataset.},
archivePrefix = {arXiv},
arxivId = {1706.09262},
author = {Kosiorek, Adam R. and Bewley, Alex and Posner, Ingmar},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.09262},
month = jun,
title = {{Hierarchical Attentive Recurrent Tracking}},
url = {http://arxiv.org/abs/1706.09262},
year = {2017}
}
@inproceedings{Mnih2014,
abstract = {Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference model gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.0030v2},
author = {Mnih, Andriy and Gregor, Karol},
booktitle = {International Conference on Machine Learning},
eprint = {1402.0030v2},
isbn = {9781634393973},
keywords = {belief networks,deep learning,variational inference},
month = jan,
title = {{Neural Variational Inference and Learning in Belief Networks}},
url = {http://arxiv.org/abs/1402.0030},
year = {2014}
}
@article{Santoro2017,
abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
archivePrefix = {arXiv},
arxivId = {1706.01427},
author = {Santoro, Adam and Raposo, David and Barrett, David G.T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
eprint = {1706.01427},
journal = {CoRR},
month = jun,
pages = {1--16},
title = {{A simple neural network module for relational reasoning}},
url = {http://arxiv.org/abs/1706.01427 https://arxiv.org/abs/1706.01427},
year = {2017}
}
@inproceedings{Mnih2016,
abstract = {Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2016) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multi-sample objectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator proposed for the single-sample variational objective, and is competitive with the currently used biased estimators.},
archivePrefix = {arXiv},
arxivId = {1602.06725},
author = {Mnih, Andriy and Rezende, Danilo J.},
booktitle = {International Conference on Machine Learning},
eprint = {1602.06725},
issn = {1938-7228},
month = feb,
title = {{Variational inference for Monte Carlo objectives}},
url = {http://arxiv.org/abs/1602.06725},
year = {2016}
}
@inproceedings{Burda2016,
abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
archivePrefix = {arXiv},
eprint = {1509.00519},
author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
booktitle = {International Conference on Learning Representations},
issn = {1312.6114v10},
month = sep,
title = {{Importance Weighted Autoencoders}},
url = {http://arxiv.org/abs/1509.00519},
year = {2016}
}
@inproceedings{Chung2015,
abstract = {In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamic hidden state.},
archivePrefix = {arXiv},
arxivId = {1506.02216},
author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1506.02216},
issn = {10495258},
month = jun,
title = {{A Recurrent Latent Variable Model for Sequential Data}},
url = {http://arxiv.org/abs/1506.02216},
year = {2015}
}
@inproceedings{Krizhevsky2012,
author = {Krizhevsky, A. and Sutskever, I. and Hinton, G. E.},
booktitle = {Advances in Neural Information Processing Systems},
pages = {1097--1105},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://papers.Advances in Neural Information Processing Systems.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks},
year = {2012}
}
@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Y. and Bengio, Y. and Hinton, G. E.},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{kemp2008discovery,
  title={The discovery of structural form},
  author={Kemp, Charles and Tenenbaum, Joshua B},
  journal={Proceedings of the National Academy of Sciences},
  volume={105},
  number={31},
  pages={10687--10692},
  year={2008},
  publisher={National Acad Sciences}
}
@article{lecun1989backpropagation,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}
@article{cho2015unsupervised,
  title={Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals},
  author={Cho, Minsu and Kwak, Suha and Schmid, Cordelia and Ponce, Jean},
  journal={CoRR},
%   volume={abs/1501.06170},
  archivePrefix = {arXiv},
  eprint = {1501.06170},
  year={2015}
}
@inproceedings{kwak2015unsupervised,
  title={Unsupervised object discovery and tracking in video collections},
  author={Kwak, Suha and Cho, Minsu and Laptev, Ivan and Ponce, Jean and Schmid, Cordelia},
  booktitle={ICCV},
  pages={3173--3181},
  year={2015},
  organization={IEEE}
}
@inproceedings{xiao2016track,
  title={Track and segment: An iterative unsupervised approach for video object proposals},
  author={Xiao, Fanyi and Jae Lee, Yong},
  booktitle={CVPR},
  pages={933--942},
  year={2016}
}
@article{babaeizadeh2017stochastic,
  title={Stochastic Variational Video Prediction},
  author={Mohammad Babaeizadeh and Chelsea Finn and Dumitru Erhan and Roy H. Campbell and Sergey Levine},
  journal={CoRR},
  year={2017},
    archivePrefix = {arXiv},
    eprint = {1710.11252},
}
@article{tulyakov2017mocogan,
  title={Mocogan: Decomposing motion and content for video generation},
  author={Tulyakov, Sergey and Liu, Ming-Yu and Yang, Xiaodong and Kautz, Jan},
  journal={CoRR},
    archivePrefix = {arXiv},
eprint = {1707.04993},
  year={2017}
}
@inproceedings{denton2017unsupervised,
  title={Unsupervised learning of disentangled representations from video},
  author={Denton, Emily L and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4417--4426},
  year={2017}
}
@article{ranzato2014video,
  title={Video (language) modeling: a baseline for generative models of natural videos},
  author={Ranzato, MarcAurelio and Szlam, Arthur and Bruna, Joan and Mathieu, Michael and Collobert, Ronan and Chopra, Sumit},
  journal={CoRR},
    archivePrefix = {arXiv},
eprint = {1412.6604},
  year={2014}
}
@inproceedings{srivastava2015unsupervised,
  title={Unsupervised learning of video representations using lstms},
  author={Srivastava, Nitish and Mansimov, Elman and Salakhudinov, Ruslan},
  booktitle={International Conference on Machine Learning},
  pages={843--852},
  year={2015}
}
@article{neiswanger2012unsupervised,
  title={Unsupervised Detection and Tracking of Arbitrary Objects with Dependent Dirichlet Process Mixtures},
  author={Neiswanger, Willie and Wood, Frank},
  journal={CoRR},
    archivePrefix = {arXiv},
eprint = {1210.3288},
  year={2012}
}
@inproceedings{weber2017imagination,
  title={Imagination-augmented agents for deep reinforcement learning},
  author={Weber, Th{\'e}ophane and Racani{\`e}re, S{\'e}bastien and Reichert, David P and Buesing, Lars and Guez, Arthur and Rezende, Danilo Jimenez and Badia, Adria Puigdom{\`e}nech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}
@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
@inproceedings{ristani2016performance,
  title={Performance measures and a data set for multi-target, multi-camera tracking},
  author={Ristani, Ergys and Solera, Francesco and Zou, Roger and Cucchiara, Rita and Tomasi, Carlo},
  booktitle={ECCV},
  pages={17--35},
  year={2016},
  organization={Springer}
}
@misc{tieleman2012rms,
  title={{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, T. and Hinton, G. E.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}
@inproceedings{maddison2017filtering,
  title={Filtering Variational Objectives},
  author={Maddison, Chris J and Lawson, John and Tucker, George and Heess, Nicolas and Norouzi, Mohammad and Mnih, Andriy and Doucet, Arnaud and Teh, Yee},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6576--6586},
  year={2017}
}
@article{gulrajani2016pixelvae,
  title={Pixelvae: A latent variable model for natural images},
  author={Gulrajani, Ishaan and Kumar, Kundan and Ahmed, Faruk and Taiga, Adrien Ali and Visin, Francesco and Vazquez, David and Courville, Aaron},
  journal={CoRR},
    archivePrefix = {arXiv},
eprint = {1611.05013},
  year={2016}
}
@inproceedings{kim2018disentangling,
  title={Disentangling by factorising},
  author={Kim, Hyunjik and Mnih, Andriy},
  journal={International Conference on Machine Learning},
    archivePrefix = {arXiv},
eprint = {1802.05983},
  year={2018}
}
@misc{itseez2015opencv,
  title={Open Source Computer Vision Library},
  author={Itseez},
  year={2015},
  howpublished = {\url{https://github.com/itseez/opencv}}
}
@article{Clevert2015elu,
  title={Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
  author={Djork-Arn{\'e} Clevert and Thomas Unterthiner and Sepp Hochreiter},
  journal={CoRR},
  year={2015},
%   volume={abs/1511.07289}
  archivePrefix = {arXiv},
eprint = {1511.07289},
  
}
@inproceedings{schulter2017deepnf,
  title={Deep Network Flow for Multi-object Tracking},
  author={Samuel Schulter and Paul Vernaza and Wongun Choi and Manmohan Krishna Chandraker},
  booktitle={CVPR},
  year={2017},
  pages={2730-2739}
}
@inproceedings{bewley2016sort,
  title={Simple online and realtime tracking},
  author={Alex Bewley and ZongYuan Ge and Lionel Ott and Fabio Tozeto Ramos and Ben Upcroft},
  booktitle={ICIP},
  year={2016},
  pages={3464-3468}
}
@inproceedings{valmadre2017end,
abstract = {The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.},
archivePrefix = {arXiv},
arxivId = {1704.06036},
author = {Valmadre, Jack and Bertinetto, Luca and Henriques, Jo{\~{a}}o F. and Vedaldi, Andrea and Torr, Philip H. S.},
booktitle = {CVPR},
eprint = {1704.06036},
title = {{End-to-end representation learning for Correlation Filter based tracking}},
opturl = {http://arxiv.org/abs/1704.06036},
year = {2017}
}
@inproceedings{karl2017dvbf,
abstract = {We introduce Deep Variational Bayes Filters (DVBF), a new methhttps://arxiv.org/pdf/1605.06432.pdfod for unsupervised learning of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions by means of variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.},
archivePrefix = {arXiv},
arxivId = {1605.06432},
author = {Karl, Maximilian and Soelch, Maximilian and Bayer, Justin and van der Smagt, Patrick},
booktitle = {International Conference on Learning Representations},
eprint = {1605.06432},
file = {:Users/adam/Documents/Mendeley/Karl et al. - 2017 - Deep Variational Bayes Filters Unsupervised Learning of State Space Models from Raw Data.pdf:pdf},
title = {{Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data}},
opturl = {http://arxiv.org/abs/1605.06432 https://arxiv.org/pdf/1605.06432.pdf},
year = {2017}
}
@inproceedings{ondruska2016deepts,
  title={Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks},
  author={Peter Ondruska and Ingmar Posner},
  booktitle={AAAI},
  year={2016}
}
@article{shi2016subpixel,
  title={Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network},
  author={Wenzhe Shi and Jose Caballero and Ferenc Huszar and Johannes Totz and Andrew P. Aitken and Rob Bishop and Daniel Rueckert and Zehan Wang},
  journal={CVPR},
  year={2016},
  pages={1874-1883}
}
@article{kingma2015adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={International Conference on Learning Representations},
  year={2015},
%   volume={abs/1412.6980}
    archivePrefix = {arXiv},
eprint = {1412.6980},
}
@inproceedings{zaheer2017deeps,
  title={Deep Sets},
  author={Manzil Zaheer and Satwik Kottur and Siamak Ravanbakhsh and Barnab{\'a}s P{\'o}czos and Ruslan R. Salakhutdinov and Alexander J. Smola},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}
@article{ha2018worldm,
  title={World Models},
  author={David Ha and J{\"u}rgen Schmidhuber},
  journal={CoRR},
  year={2018},
%   volume={abs/1803.10122}
    archivePrefix = {arXiv},
eprint = {1603.10122},
}
@inproceedings{jacobsen2016struc,
abstract = {Learning powerful feature representations with CNNs is hard when training data are limited. Pre-training is one way to overcome this, but it requires large datasets suffi-ciently similar to the target domain. Another option is to de-sign priors into the model, which can range from tuned hy-perparameters to fully engineered representations like Scat-tering Networks. We combine these ideas into structured receptive field networks, a model which has a fixed filter basis and yet retains the flexibility of CNNs. This flexibil-ity is achieved by expressing receptive fields in CNNs as a weighted sum over a fixed basis which is similar in spirit to Scattering Networks. The key difference is that we learn arbitrary effective filter sets from the basis rather than mod-eling the filters. This approach explicitly connects clas-sical multiscale image analysis with general CNNs. With structured receptive field networks, we improve consider-ably over unstructured CNNs for small and medium dataset scenarios as well as over Scattering for large datasets. We validate our findings on ILSVRC2012, Cifar-10, Cifar-100 and MNIST. As a realistic small dataset example, we show state-of-the-art classification results on popular 3D MRI brain-disease datasets where pre-training is difficult due to a lack of large public datasets in a similar domain.},
author = {Jacobsen, J{\"{o}}rn-Henrik and {Van Gemert}, Jan and Lou, Zhongyou and Smeulders, Arnold W M},
booktitle = {CVPR},
file = {:Users/adam/Documents/Mendeley/Jacobsen et al. - 2016 - Structured Receptive Fields in CNNs.pdf:pdf},
title = {{Structured Receptive Fields in CNNs}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016/papers/Jacobsen{\_}Structured{\_}Receptive{\_}Fields{\_}CVPR{\_}2016{\_}paper.pdf},
year = {2016}
}
@inproceedings{oord2016cond,
abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
archivePrefix = {arXiv},
arxivId = {1606.05328},
author = {van den Oord, Aaron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1606.05328},
month = {jun},
title = {{Conditional Image Generation with PixelCNN Decoders}},
url = {http://arxiv.org/abs/1606.05328},
year = {2016}
}

@article{Oord2018cpc,
  title={Representation Learning with Contrastive Predictive Coding},
  author={A{\"a}ron van den Oord and Yazhe Li and Oriol Vinyals},
  journal={CoRR},
  year={2018},
  volume={abs/1807.03748}
}

@article{Comon1994ica,
  title={Independent component analysis, A new concept?},
  author={Pierre Comon},
  journal={Signal Processing},
  year={1994},
  volume={36},
  pages={287-314}
}

@article{Hyvrinen1999nica,
  title={Nonlinear independent component analysis: Existence and uniqueness results},
  author={Aapo Hyv{\"a}rinen and Petteri Pajunen},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={1999},
  volume={12 3},
  pages={
          429-439
        }
}

@article{Hyvrinen2018cica,
  title={Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning},
  author={Aapo Hyv{\"a}rinen and Hiroaki Sasaki and Richard E. Turner},
  journal={CoRR},
  year={2018},
  volume={abs/1805.08651}
}

@inproceedings{Hyvrinen2017tica,
  title={Nonlinear ICA of Temporally Dependent Stationary Sources},
  author={Aapo Hyv{\"a}rinen and Hiroshi Morioka},
  booktitle={AISTATS},
  year={2017}
}

@inproceedings{Rasmus2015ladder,
  title={Semi-supervised Learning with Ladder Networks},
  author={Antti Rasmus and Mathias Berglund and Mikko Honkala and Harri Valpola and Tapani Raiko},
  booktitle={Advances in Neural Information Processing Systems},
  year={2015}
}


@inproceedings{Sabour2017capsule,
  title={Dynamic Routing Between Capsules},
  author={Sabour, Sara and Frosst, Nick and Hinton, G. E.},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}


@inproceedings{Hinton2018capsule,
  title={Matrix Capsules with EM routing},
  author={Hinton, G. E. and Sabour, Sara and Frosst, Nick},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@article{Graves2016mixture,
  title={Stochastic Backpropagation through Mixture Density Distributions},
  author={Alex Graves},
  journal={CoRR},
  year={2016},
  volume={abs/1607.05690}
}

@article{Jang2016gumbel,
  title={Categorical Reparameterization with Gumbel-Softmax},
  author={Eric Jang and Shixiang Gu and Ben Poole},
  journal={CoRR},
  year={2016},
  volume={abs/1611.01144}
}

@article{Maddison2016concrete,
  title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
  author={Chris J. Maddison and Andriy Mnih and Yee Whye Teh},
  journal={CoRR},
  year={2016},
  volume={abs/1611.00712}
}

@inproceedings{Maale2016auxdgm,
  title={Auxiliary Deep Generative Models},
  author={Maal{\o}e, Lars and S{\o}nderby, Casper Kaae and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  booktitle={International Conference on Machine Learning},
  year={2016}
}

@inproceedings{Sonderby2016ladder,
  title={Ladder Variational Autoencoders},
  author={Casper Kaae S{\o}nderby and Tapani Raiko and Lars Maal{\o}e and S{\o}ren Kaae S{\o}nderby and Ole Winther},
  booktitle={Advances in Neural Information Processing Systems},
  year={2016}
}

@inproceedings{Maaloe2019biva,
  title={BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling},
  author={Lars Maal{\o}e and Marco Fraccaro and Valentin Li'evin and Ole Winther},
  year={2019}
}

@inproceedings{Vaswani2017transformer,
  title={Attention Is All You Need},
  author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{Parmar2018imtransf,
  title={Image Transformer},
  author={Niki Parmar and Ashish Vaswani and Jakob Uszkoreit and Lukasz Kaiser and Noam Shazeer and Alexander Ku},
  booktitle={International Conference on Machine Learning},
  year={2018}
}

@inproceedings{Lee2019set,
  title={Set Transformer},
  author={Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam R and Choi, Seungjin and Teh, Yee Whye},
  booktitle={International Conference on Machine Learning},
  eprint={1810.00825},
  archivePrefix = {arXiv},
  year={2019},
}

@inproceedings{Golovin2017vizier,
  title={Google vizier: A service for black-box optimization},
  author={Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John and Sculley, D},
  booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={1487--1495},
  year={2017},
  organization={ACM}
}

@book{Tieleman2014thesis,
  title={Optimizing Neural Networks That Generate Images},
  author={Tieleman, Tijmen},
  year={2014},
  publisher={University of Toronto, Canada}
}

@inproceedings{Maddison2017concrete,
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  title = {{The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}},
  booktitle = {International Conference on Learning Representations},
  year = {2017}
}

@inproceedings{Hinton2011tae,
  title={Transforming Auto-Encoders},
  author = {Hinton, G. E. and Krizhevsky, A. and Wang, S. D.},
  booktitle={International Conference on Artifical Neural Networks},
  year={2011}
}

@article{lalonde,
  title={Capsules for object segmentation},
  author={LaLonde, Rodney and Bagci, Ulas},
  journal={arXiv preprint arXiv:1804.04241},
  year={2018}
}

@inproceedings{lenssen,
  title={Group Equivariant Capsule Networks},
  author={Lenssen, Jan Eric and Fey, Matthias and Libuschewski, Pascal},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8844--8853},
  year={2018}
}

@inproceedings{duarte,
  title={Videocapsulenet: A simplified network for action detection},
  author={Duarte, Kevin and Rawat, Yogesh and Shah, Mubarak},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7610--7619},
  year={2018}
}

@article{encapsule,
  author    = {Hongyang Li and
              Xiaoyang Guo and
              Bo Dai and
              Wanli Ouyang and
              Xiaogang Wang},
  title     = {Neural Network Encapsulation},
  journal   = {CoRR},
%   volume    = {abs/1808.03749},
  year      = {2018},
%   url       = {http://arxiv.org/abs/1808.03749},
  archivePrefix = {arXiv},
  eprint    = {1808.03749},
%   timestamp = {Sun, 02 Sep 2018 15:01:55 +0200},
%   biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1808-03749},
%   bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhang2018fast,
  title={Fast Dynamic Routing Based on Weighted Kernel Density Estimation},
  author={Zhang, Suofei and Zhou, Quan and Wu, Xiaofu},
  booktitle={International Symposium on Artificial Intelligence and Robotics},
  pages={301--309},
  year={2018},
%   organization={Springer}
}

@inproceedings{wang2018optimization,
  title={An Optimization View on Dynamic Routing Between Capsules},
  author={Wang, Dilin and Liu, Qiang},
  journal={ICLR Workshop},
  year={2018}
}

@article{saqur,
  title={CapsGAN: Using dynamic routing for generative adversarial networks},
  author={Saqur, Raeid and Vivona, Sal},
  journal={arXiv preprint arXiv:1806.03968},
  year={2018}
}

@inproceedings{jaiswal,
  title={Capsulegan: Generative adversarial capsule network},
  author={Jaiswal, Ayush and AbdAlmageed, Wael and Wu, Yue and Natarajan, Premkumar},
  booktitle={European Conference on Computer Vision (ECCV)},
%   pages={0--0},
  year={2018}
}

@article{upadhyay,
  title={Generative adversarial network architectures for image synthesis using capsule networks},
  author={Upadhyay, Yash and Schrater, Paul},
  journal={arXiv preprint arXiv:1806.03796},
  year={2018}
}

@article{zhao20183d,
  title={3D Point-Capsule Networks},
  author={Zhao, Yongheng and Birdal, Tolga and Deng, Haowen and Tombari, Federico},
  journal={arXiv preprint arXiv:1812.10775},
  year={2018}
}

@inproceedings{mallat,
  title={Deep Roto-Translation Scattering for Object Classification},
  author={Oyallon, Edouard and Mallat, St{\'e}phane},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2865--2873},
  year={2015}
}

@article{sparsecaps,
  title={Sparse Unsupervised Capsules Generalize Better},
  author={Rawlinson, David and Ahmed, Abdelrahman and Kowadlo, Gideon},
  journal={CoRR},
  archivePrefix = {arXiv},
  eprint={1804.06094},
  year={2018}
}

@article{kocvok,
  title={Exploiting Cyclic Symmetry in Convolutional Neural Networks},
  author={Dieleman, Sander and De Fauw, Jeffrey and Kavukcuoglu, Koray},
  journal={CoRR},
  archivePrefix = {arXiv},
  eprint = {1602.02660},
  year={2016}
}

@inproceedings{imsat,
  title={Learning Discrete Representations via Information Maximizing Self-augmented Training},
  author={Hu, Weihua and Miyato, Takeru and Tokui, Seiya and Matsumoto, Eiichi and Sugiyama, Masashi},
  booktitle={International Conference on Machine Learning},
  pages={1558--1567},
  year={2017},
%   organization={JMLR. org}
}

@article{iic,
  author    = {Xu Ji and
              Jo{\~{a}}o F. Henriques and
              Andrea Vedaldi},
  title     = {Invariant Information Distillation for Unsupervised Image Segmentation
              and Clustering},
  journal   = {CoRR},
%   volume    = {abs/1807.06653},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.06653},
  archivePrefix = {arXiv},
  eprint    = {1807.06653},
%   timestamp = {Mon, 13 Aug 2018 16:48:39 +0200},
%   biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-06653},
%   bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gan,
  title={Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
  author={Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal={International Conference on Learning Representations},
  year={2016}
}

@inproceedings{ae,
  title={Greedy Layer-wise Training of Deep Networks},
  author={Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={153--160},
  year={2007}
}

@inproceedings{adc,
  title={Associative Deep Clustering: Training a Classification Network with No Labels},
  author={Haeusser, Philip and Plapp, Johannes and Golkov, Vladimir and Aljalbout, Elie and Cremers, Daniel},
  booktitle={German Conference on Pattern Recognition},
  pages={18--32},
  year={2018},
%   organization={Springer}
}

@article{Jacobsen2017dynamic,
  title={Dynamic steerable blocks in deep residual networks},
  author={Jacobsen, J{\"o}rn-Henrik and De Brabandere, Bert and Smeulders, Arnold WM},
  journal={CoRR},
  archivePrefix = {arXiv},
  eprint={1706.00598},
  year={2017}
}

@inproceedings{Gutmann2010nce,
  title={Noise-contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models},
  author={Gutmann, Michael and Hyv{\"a}rinen, Aapo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={297--304},
  year={2010}
}

@inproceedings{Cohen2016group,
  title={Group Equivariant Convolutional Networks},
  author={Cohen, Taco and Welling, Max},
  booktitle={International Conference on Machine Learning},
  pages={2990--2999},
  year={2016}
}

@inproceedings{Cohen2016steerable,
  title={Steerable CNNs},
  author={Cohen, Taco and Welling, Max},
  booktitle={International Conference on Representation Learning},
  year={2017}
}
@article{Hjelm2019deepinfomax,
  title={Learning Deep Representations by Mutual Information Estimation and Maximization},
  author={R. Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Adam Trischler and Yoshua Bengio},  
  journal={CoRR},
  archivePrefix = {arXiv},
  year={2019},
  eprint={1808.06670}
}

@book{Rock73,
  title={Orientation and form},
  author={Rock, Irvin},
  year={1973},
  publisher={Academic Press}
}

@article{Hinton79,
  author = 	 {Hinton, G. E.},
  title = 	 {Some Demonstrations of the Effects of Structural Descriptions in Mental Imagery},
  journal =  {Cognitive Science},
  year = 	 {1979},
  volume = 	 {3},
  pages = 	 {231--250},
}

@inproceedings{Kosiorek2018sqair,
  title={Sequential Attend, Infer, Repeat: Generative modelling of moving objects},
  author={Kosiorek, Adam and Kim, Hyunjik and Teh, Yee Whye and Posner, Ingmar},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8606--8616},
  archivePrefix = {arXiv},
  eprint = {1806.01794},
  year={2018}
}

@article{Burgess2019monet,
  title={MONet: Unsupervised Scene Decomposition and Representation},
  author={Burgess, Christopher P and Matthey, Loic and Watters, Nicholas and Kabra, Rishabh and Higgins, Irina and Botvinick, Matt and Lerchner, Alexander},
  journal={CoRR},
  archivePrefix = {arXiv},
  eprint = {1901.11390},
  year={2019}
}

@article{Greff2019multi,
  title={Multi-Object Representation Learning with Iterative Variational Inference},
  author={Greff, Klaus and Kaufmann, Rapha{\"e}l Lopez and Kabra, Rishab and Watters, Nick and Burgess, Chris and Zoran, Daniel and Matthey, Loic and Botvinick, Matthew and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1903.00450},
  year={2019}
}

@article{Kuhn1955hungarian,
  title={The Hungarian Method for the Assignment Problem},
  author={Kuhn, Harold W},
  journal={Naval Research Logistics Quarterly},
%   volume={2},
%   number={1-2},
  pages={83--97},
  year={1955},
  publisher={Wiley Online Library}
}

@article{liu2019soft,
  title={Soft Rasterizer: Differentiable Rendering for Unsupervised Single-View Mesh Reconstruction},
  author={Liu, Shichen and Chen, Weikai and Li, Tianye and Li, Hao},
  journal={CoRR},
  archivePrefix = {arXiv},
  eprint = {1901.05567},
  year={2019}
}

@article{Ba2016LayerN,
  title={Layer Normalization},
  author={Ba, Jimmy and Kiros, Jamie and Hinton, G E},
  year={2016},
  journal={CoRR},
  archivePrefix = {arXiv},
  eprint = {1607.06450},
}

% @article{Ba2016LayerN,
%   title={Layer Normalization},
%   author={Jimmy Ba and Jamie Kiros and G E Hinton},
%   journal={CoRR},
%   year={2016},
%   volume={abs/1607.06450}
% }