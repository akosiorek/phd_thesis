%Previous capsule networks use the poses of parts and the discriminatively learned part->object relationships to cast votes for the poses of objects and then look for clusters among these votes.



% \yw{edited for anonymity}
%Capsule networks\footnote{A \textit{capsule} is a group of neurons representing a single concept, whose output is a feature vector; the term can sometimes be used to refer to the vector, too.} \citep{Hinton2018capsule} use this insight by modelling object parts explicitly. They predict the affine transformations between object parts \apart{} and camera \acamera{} coordinate frames (parts' \textit{pose}) and learn transformations between part \apart{} and object \awhole{} coordinate frames.
% \todo{\ak{don't remove the icons! they help me understand text and I'm sure others will appreciate, too}}


%Capsules do have several drawbacks, however, related to how they predict whole objects: each part makes a separate prediction for a whole, which is assumed to be present when predictions of many parts agree.
%

% \ak{yeah, like we don't have hyperparameters...}
% (i) they are sensitive to the threshold for how tightly the predictions of the parts should agree,
%(i) capsules use iterative inference to ensure that a part is included in only one whole;
% (ii) the pose of each part needs to have the same dimensionality as the pose of the whole if it is to make a point prediction for the latter, and
%(ii) since every part \apart{} has to make a point prediction for the pose of a whole \awhole{}, both poses need to have the same dimensionality; and
%(iii) learning is supervised.
\vspace{-1pt}
\section{Related Work}
\label{sec:related_work}\vspace{-5pt}

\textbf{Capsule Networks}\ \ 
Our work combines ideas from Transforming Autoencoders \citep{Hinton2011tae} and \textsc{em} Capsules \citep{Hinton2018capsule}.
Transforming autoencoders discover affine-aware capsule \textit{instantiation parameters} by training an autoencoder to predict an affine-transformed version of the input image from the original image plus an extra input, which explicitly represents the transformation.
By contrast, our model does not need any input other than the image.

Both \textsc{em} Capsules and the preceding Dynamic Capsules \citep{Sabour2017capsule} use the poses of parts and learned part$\rightarrow$object relationships to vote for the poses of objects. When multiple parts cast very similar votes, the object is assumed to be present, which is facilitated by an interactive inference (routing) algorithm. Iterative routing is inefficient and has prompted further research. \cite{Wang2018optimization} formulated routing as an optimization of a clustering loss and a \textsc{kl}-divergence-based regularization term.  \cite{Zhang2018fast} proposed a weighted kernel density estimation-based routing method. \cite{Encapsule} proposed approximating routing with two branches and sending feedback via optimal transport divergence between two distributions (lower and higher capsules). 
In contrast to prior work, we use objects to predicts parts rather than vice-versa, therefore we can dispense with iterative routing at inference time. The encoder of the \gls{OCAu} learns how to group parts into objects and it respects the single parent constraint, because it is trained using derivatives produced by a decoder that uses a mixture model of parts which assumes that each part must be explained by a single object. 

Additionally, since it is the objects that predict parts, the parts are allowed to have fewer degrees-of-freedom in their poses than objects (as in the \gls{CCAu}). 
Inference is still possible, because the \gls{OCAu} encoder makes object predictions based on {\it all} the parts rather than an individual part.

A further advantage of our version of capsules is that it can perform unsupervised learning. Previous versions of capsules used discriminative learning, though \cite{Sparsecaps} used the reconstruction \gls{MLP} introduced in \cite{Sabour2017capsule} to train Dynamic Capsules without supervision and has shown that unsupervised training for capsule-conditioned reconstruction helps with generalization to \textsc{affnist} classification; we further improve on their results, \textit{cf}.\ \Cref{sec:ablation}.

% \comment{ by maximizing the likelihood of part poses under mixtures of predictions made by different object capsules.
% Moreover, our method is trained without supervision while the previous approaches require supervised training.}
% 
% \ak{}{the following is irrelevant to this work}
% \cite{Lalonde} proposed a deconvolutional capsule network for object segmentation and \cite{Duarte} introduced capsule pooling for action segmentation and classification. 
% \cite{Jaiswal,Upadhyay,Saqur} developed Capsule\textsc{gan}s, which have shown better 2D image generation quality.

% \cite{Zhao20183d} proposed Capsules as disjoint latent basis functions to summarize point clouds and learned the embedded latent capsules via dynamic routing.
% These are similar to our method, with the difference that we use an affine-aware capsule decoder to replace routing and reconstruct images.

\textbf{Unsupervised Classification}\ \ 
There are two main approaches to unsupervised object category detection in computer vision.
The first one is based on representation learning and typically requires discovering clusters or learning a classifier on top of the learned representation.
\cite{Eslami2016,Kosiorek2018sqair} use an iterative procedure to infer a variable number of latent variables, one for every object in a scene, that are highly informative of object class, while \cite{Greff2019multi,Burgess2019monet} perform unsupervised instance-level segmentation in an iterative fashion.
While similar to our work, these approaches cannot decompose objects into their constituent parts and do not provide explicit description of object shape (\!\eg templates and their poses in our model).

The second approach targets classification explicitly by minimizing mutual information (\textsc{mi})-based losses and directly learning class-assignment probabilities.
\textsc{Iic} \citep{Iic} maximizes an exact estimator of \textsc{mi} between two discrete probability vectors describing (transformed) versions of the input image.
DeepInfoMax \citep{Hjelm2019deepinfomax} relies on negative samples and maximizes \textsc{mi} between the predicted probability vector and its input via noise-contrastive estimation \citep{Gutmann2010nce}.
This class of methods directly maximizes the amount of information contained in an assignment to discrete clusters and they hold state-of-the-art results on most unsupervised classification tasks.
\textsc{Mi}-based methods suffer from typical drawbacks of mutual information estimation: they require heavy data augmentation and large batch sizes.
This is in contrast to our method, which achieves comparable performance with batch size no bigger than 128 and with no data augmentation.

\textbf{Geometrical Reasoning}\ \ 
Other attempts at incorporating geometrical knowledge into neural networks include exploiting equivariance properties of group transformations \citep{Cohen2016group} or new types of convolutional filters \citep{Mallat,Kocvok}.
Although they achieve significant parameter efficiency in handling rotations or reflections compared to standard \glspl{CNN}, these methods cannot handle additional degrees of freedom of affine transformations---like scale. \cite{Lenssen} combined capsule networks with group convolutions to  guarantee equivariance and invariance in capsule networks.
Spatial Transformers (\textsc{st}; \cite{Jaderberg2015}) apply affine transformations to the image sampling grid while steerable networks \citep{Cohen2016steerable,Jacobsen2017dynamic} dynamically change convolutional filters.
These methods are similar to ours in the sense that transformation parameters are predicted by a neural network, but differ in the sense that \textsc{st} uses global transformations applied to the whole image while steerable networks use only local transformations.
Our approach can use different global transformations for every object as well as local transformations for each of their parts.

% \ak{}{I think we can skip GQN and HoloGAN}
% Novel view synthesis is an emerging task which requires affine-aware representation learning. 
% \Gls{GQN} \citep{Eslami2016} is closely related to transforming autoencoders: it learns to represent a whole three-dimensional scene in its latent variable computed from multiple viewpoints and trains an affine-aware decoder to reconstruct the scene from a given viewpoint.
% More recently HoloGAN \citep{hologan} has been able to generate new viewpoints without any supervision. HoloGAN applies random 3D affine transformations on learned templates to disentangle pose from shape and therefore is able to generate new viewpoints of the same shape.



