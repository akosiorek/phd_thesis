\chapter{Literature Review}
\label{ch:lit_review}

	Learning object-centric representations lies at the intersection of several areas of machine learning research. 
	Concretely, these are \begin{inparaenum}[(1)\!]
	\item relational reasoning, because having representations of objects can facilitate reasoning about their relations, but also because we can discover objects by analysing relations between different elements of the scene,
	\item object detection and tracking, as these two areas of research focus on locating objects in images and videos, respectively,
	\item representation learning, since the objective of our task it to learn representations, albeit ones that describe objects and not entire scenes, and finally
	\item compositional generative modelling, because object-centric representations can be useful for this task, but also because this area of research is among the most-promising candidates for learning object-centric representations.
	\end{inparaenum}
	We now describe these four areas in their dedicated sections, starting with relational reasoning.


\section{Relational Reasoning}
\label{sec:relational_reasoning}

	We define relational reasoning as making inferences about a group of entities such as physical objects with certain properties (shape, mass) that exist in a shared context.
	 Importantly, we can make inferences about relations between any objects in a group, or we can use the knowledge of such relations to answer other (externally-posed) questions about the entities.
	 This definition closely follows that of \cite{Battaglia2018relnets}, who also present an overview of this field of research. 
	 A typical example of relational reasoning is that of inferring which object in a scene is the biggest, or counting all objects made of metal \citep{Santoro2017}.
	 In a more complicated example, agents can learn the relations between objects in the environment, and use some objects to indirectly interact with other objects \citep{Baker2019tooluse}.
	 If relations between objects are known, one can use this knowledge to build a \gls{GNN} \citep{Schlichtkrull2017relgraph} whose structure reflects these relations, and where different types of relations can be processed by dedicated neural net modules.
	 If the number of entities is too large, or if relations only between neighbouring entities are known, one can process the graph of entities iteratively using message passing.
	 Alternatively, especially if relations between all objects exist or if their nature is not known, one could consider self-attention \citep{Vaswani17}, which performs relational reasoning on a fully-connected graph of entities.
	 The downside is the quadratic (in the number of entities) computational complexity of this approach.
	 Fortunately, this can be improved to linear computational complexity by using inducing points \citep{Lee2019set} or local self-attention \citep{Ramachandran2019standalone}.
	 
	 Relational reasoning can be useful for inferring which parts of the scene constitute objects, but in order to reason about relations, one has to know what entities are present in the scene. 
	 Typically, this data is not available and one has to resort to heuristics solutions.
	 \cite{Santoro2017} uses a \gls{CNN} to embed an image and assumes that each location in a feature map can correspond to an object, and then considers pairwise relations between every pair of locations.
	 \cite{Battaglia2016,Baker2019tooluse} assume access to the ground-truth state of different entities,
	 and \cite{Yi2019cleverer} use \textsc{mask r-cnn} \citep{He2017maskrcnn} for segmenting out different objects before feeding them into a relational reasoning module.


\section{Object Detection and Tracking}
\label{sec:object_det_track}

	In order to reason about relations between objects in an environment, we first need to know what objects exist in that environment and where they are.
	It is also useful to know their temporal characteristics,\ie how they move, how their appearance changes or if they are attracted by certain parts of the environment.
	Object detection and tracking algorithms can provide valuable information that can be used as input to relational reasoning algorithms, but the extracted information is typically limited to the the location and possibly the history of object location.
	Given the location, it is possible to extract appearance information and possibly other characteristics, but this requires complicated post-processing.
	Alternatively, one could use an end-to-end tracking algorithm that is closely integrated with the relational reasoning module, but as the following overview shows, this is not possibly with the current state-of-the-art object tracking methods.
	
	%attention in tracking  
	In the context of single object tracking, both attention mechanisms and RNNs appear to be perfectly suited, yet their success has mostly been limited to simple monochromatic sequences with plain backgrounds \cite{Kahou2015ratm}. \citet{Cheung2016gtc} applied STNs \cite{Jaderberg2015} as attention mechanisms for real-world object tracking, but failed due to exploding gradients potentially arising from the difficulty of the data.
	\citet{Ning2016} achieved competitive performance by using features from an object detector as inputs to a long-short memory network (LSTM), but requires processing of the whole image at each time-step. 
	
	Two recent state-of-the-art trackers employ convolutional Siamese networks which can be seen as an RNN unrolled over two time-steps \cite{Held2016goturn, Valmadre2017}. Both methods explicitly process small search areas around the previous target position to produce a bounding box offset \cite{Held2016goturn} or a correlation response map with the maximum corresponding to the target position \cite{Valmadre2017}. 
	We acknowledge the recent work\footnote{\cite{Gordon2018re3} only became available at the time of submitting this paper.} of \citet{Gordon2018re3} which employ an RNN based model and use explicit cropping and warping as a form of non-differentiable spatial attention.
	The work presented in this paper is closest to \cite{Kahou2015ratm} where we share a similar spatial attention mechanism which is guided through an RNN to effectively learn a motion model that spans multiple time-steps. The next section describes our additional attention mechanisms in relation to their biological counterparts.
	
	\paragraph{Tracking-by-Detection} Vision-based tracking approaches typically follow a tracking-by-detection paradigm: objects are first detected in each frame independently, and then a tracking algorithm links the detections from different frames to propose a coherent trajectory \cite{Zhang2008,Milan2014,Bae2017confidence,Keuper2018motion}.
	Motion models and appearance are often used to improve the association between detected bounding-boxes and multiple trackers in a postprocessing step.
	Recently, elements of this pipeline have been replaced with learning-based approaches such as deep learning \cite{Nam2016,Ning2017,Keuper2018motion,Bae2017confidence} or reinforcement learning \cite{Xiang2015}. Some approaches are targeted towards robustness across domains, for example by using a category-agnostic object detector and performing classification only in a post-processing step \cite{Osep2017,Posner2016}.
	
	
	\paragraph{End-to-End Tracking} A newly established and much less explored stream of work approaches tracking in an end-to-end fashion. A key difficulty here is that extracting an image crop (according to bounding-boxes provided by a detector), is non-differentiable and results in high-variance gradient estimators.
	\citet{Kahou2015ratm} propose an end-to-end tracker with soft spatial-attention using a 2D grid of Gaussians instead of a hard bounding-box. \Gls{HART} draws inspiration from this idea, employs an additional attention mechanism, and shows promising performance on the real-world KITTI dataset \cite{Kosiorek2017hierch}.
	\Gls{HART}, which forms the foundation of this work, is explained in detail in \Cref{sec:method}. It has also been extended to incorporate depth information from \textsc{rgbd} cameras \cite{Danesh2019deep}. \citet{Gordon2018re3} propose an approach in which the crop corresponds to the scaled up previous bounding-box. This simplifies the approach, but does not allow the model to learn where to look---\ie no gradient is backpropagated through crop coordinates.
	To the best of our knowledge, there are no successful implementations of any such end-to-end approaches for multi-object tracking beyond \textsc{sqair} \citep{Kosiorek2018sqair}, which works only on datasets with static backgrounds. On real-world data, the only end-to-end approaches correspond to applying multiple single-object trackers in parallel---a method which does not leverage the potential of scene context or inter-object interactions. 
	
		[Object Tracking]
	There have been many approaches to modelling objects in images and videos. 
	Object detection and tracking are typically learned in a supervised manner, where object bounding boxes and often additional labels are part of the training data.
	Single-object tracking commonly use Siamese networks, which can be seen as an \gls{RNN} unrolled over two time-steps \citep{Valmadre2017}. Recently, \cite{Kosiorek2017hierch} used an \gls{RNN} with an attention mechanism in the \textsc{hart} model to predict bounding boxes for single objects, while robustly modelling their motion and appearance. Multi-object tracking is typically attained by detecting objects and performing data association on bounding-boxes \citep{Bewley2016sort}.
	\cite{Schulter2017deepnf} used an end-to-end supervised approach that detects objects and performs data association.
	In the unsupervised setting, where the training data consists of only images or videos, the dominant approach is to distill the inductive bias of spatial consistency into a discriminative model.  \cite{Cho2015unsupervised} detect single objects and their parts in images, and \cite{Kwak2015unsupervised,Xiao2016track} incorporate temporal consistency to better track single objects.
	\Gls{SQAIR} is unsupervised and hence it does not rely on bounding boxes nor additional labels for training, while being able to learn arbitrary motion and appearance models similarly to \textsc{hart} \citep{Kosiorek2017hierch}.
	At the same time, is inherently multi-object and performs data association implicitly (\textit{cf}. \Cref{app:algo}).
	Unlike the other unsupervised approaches, temporal consistency is baked into the model structure of \gls{SQAIR} and further enforced by lower \gls{KL} divergence when an object is tracked.


\section{Representation Learning and Disentanglement}
\label{sec:repr_learning}


		[Learning Decomposed Representations of Images and Videos]
	Learning decomposed representations of object appearance and position lies at the heart of our model.
	This problem can be also seen as perceptual grouping, which involves modelling pixels as spatial mixtures of entities.
	\cite{Greff2016tagger} and \cite{Greff2017neuralem} learn to decompose images into separate entities by iterative refinement of spatial clusters using either learned updates or the Expectation Maximization algorithm;
	\cite{Ilin2017recurrentln} and \cite{Steenkiste2018} extend these approaches to videos, achieving very similar results to \gls{SQAIR}.
	Perhaps the most similar work to ours is the concurrently developed model of \cite{Hsieh2018ddpae}.
	The above approaches rely on iterative inference procedures, but do not exhibit the object-counting behaviour of \gls{SQAIR}.
	For this reason, their computational complexities are proportional to the predefined maximum number of objects, while \gls{SQAIR} can be more computationally efficient by adapting to the number of objects currently present in an image.
	
	Another interesting line of work is the \textsc{gan}-based unsupervised video generation that decomposes motion and content \citep{Tulyakov2017mocogan,Denton2017unsupervised}. These methods learn interpretable features of content and motion, but deal only with single objects and do not explicitly model their locations. Nonetheless, adversarial approaches to learning structured probabilistic models of objects offer a plausible alternative direction of research.



\section{Generative Modelling}
\label{sec:gen_modelling}

	There are two main approaches to unsupervised object category detection in computer vision.
	The first one is based on representation learning and typically requires discovering clusters or learning a classifier on top of the learned representation.
	\cite{Eslami2016air,Kosiorek2018sqair} use an iterative procedure to infer a variable number of latent variables, one for every object in a scene, that are highly informative of object class, while \cite{Greff2019multi,Burgess2019monet} perform unsupervised instance-level segmentation in an iterative fashion.
	While similar to our work, these approaches cannot decompose objects into their constituent parts and do not provide explicit description of object shape (\!\eg templates and their poses in our model).
	
	The second approach targets classification explicitly by minimizing mutual information (\textsc{mi})-based losses and directly learning class-assignment probabilities.
	\textsc{Iic} \citep{Ji2018iic} maximizes an exact estimator of \textsc{mi} between two discrete probability vectors describing (transformed) versions of the input image.
	DeepInfoMax \citep{Hjelm2019deepinfomax} relies on negative samples and maximizes \textsc{mi} between the predicted probability vector and its input via noise-contrastive estimation \citep{Gutmann2010nce}.
	This class of methods directly maximizes the amount of information contained in an assignment to discrete clusters and they hold state-of-the-art results on most unsupervised classification tasks.
	\textsc{Mi}-based methods suffer from typical drawbacks of mutual information estimation: they require heavy data augmentation and large batch sizes.
	This is in contrast to our method, which achieves comparable performance with batch size no bigger than 128 and with no data augmentation.
	
	
	
	[Video Prediction]
	Many works on video prediction learn a deterministic model conditioned on the current frame to predict the future ones \citep{Ranzato2014video,Srivastava2015unsupervised}.
	Since these models do not model uncertainty in the prediction, they can suffer from the multiple futures problem --- since perfect prediction is impossible, the model produces blurry predictions which are a mean of possible outcomes.
	This is addressed in stochastic latent variable models trained using variational inference to generate multiple plausible videos given a sequence of images \citep{Babaeizadeh2017stochastic, Denton2018stochastic}.
	Unlike \gls{SQAIR}, these approaches do not model objects or their positions explicitly, thus the representations they learn are of limited interpretability. 
	


%\textbf{Capsule Networks}\ \ 
%Our work combines ideas from Transforming Autoencoders \citep{Hinton2011tae} and \textsc{em} Capsules \citep{Hinton2018capsule}.
%Transforming autoencoders discover affine-aware capsule \textit{instantiation parameters} by training an autoencoder to predict an affine-transformed version of the input image from the original image plus an extra input, which explicitly represents the transformation.
%By contrast, our model does not need any input other than the image.
%
%Both \textsc{em} Capsules and the preceding Dynamic Capsules \citep{Sabour2017capsule} use the poses of parts and learned part$\rightarrow$object relationships to vote for the poses of objects. When multiple parts cast very similar votes, the object is assumed to be present, which is facilitated by an interactive inference (routing) algorithm. Iterative routing is inefficient and has prompted further research. \cite{Wang2018optimization} formulated routing as an optimization of a clustering loss and a \textsc{kl}-divergence-based regularization term.  \cite{Zhang2018fast} proposed a weighted kernel density estimation-based routing method. \cite{Li2018encapsule} proposed approximating routing with two branches and sending feedback via optimal transport divergence between two distributions (lower and higher capsules). 
%In contrast to prior work, we use objects to predicts parts rather than vice-versa, therefore we can dispense with iterative routing at inference time. The encoder of the \gls{OCAu} learns how to group parts into objects and it respects the single parent constraint, because it is trained using derivatives produced by a decoder that uses a mixture model of parts which assumes that each part must be explained by a single object. 
%
%Additionally, since it is the objects that predict parts, the parts are allowed to have fewer degrees-of-freedom in their poses than objects (as in the \gls{CCAu}). 
%Inference is still possible, because the \gls{OCAu} encoder makes object predictions based on {\it all} the parts rather than an individual part.
%
%A further advantage of our version of capsules is that it can perform unsupervised learning. Previous versions of capsules used discriminative learning, though \cite{Rawlinson2018sparsecaps} used the reconstruction \gls{MLP} introduced in \cite{Sabour2017capsule} to train Dynamic Capsules without supervision and has shown that unsupervised training for capsule-conditioned reconstruction helps with generalization to \textsc{affnist} classification; we further improve on their results, \textit{cf}.\ \Cref{sec:ablation}.

%
%%spatial attention
%A number of recent studies have demonstrated that visual content can be captured through a sequence of spatial glimpses or foveation \cite{Graves2014recurrent, Gregor2016towards}. Such a paradigm has the intriguing property that the computational complexity is proportional to the number of steps as opposed to the image size.
%Furthermore, the fovea centralis in the retina of primates is structured with maximum visual acuity in the centre and decaying resolution towards the periphery, \citet{Olshausen2016foveal} show that if spatial attention is capable of zooming, a regular grid sampling is sufficient. 
%\citet{Jaderberg2015} introduced the spatial transformer network (STN) which provides a fully differentiable means of transforming feature maps, conditioned on the input itself. \citet{Eslami2016air} use the STN as a form of attention in combination with a recurrent neural network (RNN) to sequentially locate and identify objects in an image. Moreover, \citet{Eslami2016air} use a latent variable to estimate the presence of additional objects, allowing the RNN to adapt the number of time-steps based on the input. 
%Our spatial attention mechanism is based on the two dimensional Gaussian grid filters of  \cite{Kahou2015ratm} which is both fully differentiable and more biologically plausible than the STN.  
%%While the aforementioned approaches use a regular sampling lattice which is far simpler than of the primate retina, \citet{Olshausen2016foveal} showed that complex sampling patterns are not required when attention is able to zoom-in on a region of interest. 
%
%%appearance attention with RNNs
%Whilst focusing on a specific location has its merits, focusing on particular appearance features might be as important. A policy with feedback connections can learn to adjust filters of a convolutional neural network (CNN), thereby adapting them to features present in the current image and improving accuracy \cite{Stollenga2014}. \citet{Brabandere2016dfn} introduced dynamic filter network (DFN), where filters for a CNN are computed on-the-fly conditioned on input features, which can reduce model size without performance loss. \citet{Karl2017dvbf} showed that an input-dependent state transitions can be helpful for learning latent Markovian state-space system. While not the focus of this work, we follow this concept in estimating the expected appearance of the tracked object.

