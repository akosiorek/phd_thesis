\documentclass{article}

\include{preamble}

\usepackage{fancyhdr}
%\pagenumbering{arabic} 
%\pagestyle{fancy}
%\cfoot{\vspace*{1.5\baselineskip} \thepage}

\setlength{\parskip}{0.2em}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Tighter Variational Bounds are Not Necessarily Better}


\begin{document}
%
%	\vspace{-20pt}

	
\twocolumn[
\icmltitle{Tighter Variational Bounds are Not Necessarily Better}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
	\icmlauthor{Tom Rainforth}{stats}
\icmlauthor{Adam R. Kosiorek}{stats,eng}
\icmlauthor{Tuan Anh Le}{eng}
\icmlauthor{Chris J. Maddison}{stats}
\icmlauthor{Maximilian Igl}{eng}
\icmlauthor{Frank Wood}{ucb}
\icmlauthor{Yee Whye Teh}{stats}
\end{icmlauthorlist}

\icmlaffiliation{stats}{Department of Statistics, University of Oxford}
\icmlaffiliation{eng}{Department of Engineering, University of Oxford}
\icmlaffiliation{ucb}{Department of Computer Science, University of British Columbia}

\icmlcorrespondingauthor{Tom Rainforth}{rainforth@stats.ox.ac.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Variational inference, variational autoencoders, importance weighted autoencoders}

\vskip 0.3in
]
\printAffiliationsAndNotice{}	

%
\setlength{\abovedisplayskip}{2.5pt}
\setlength{\belowdisplayskip}{2.5pt}
\setlength{\abovedisplayshortskip}{2.5pt}
\setlength{\belowdisplayshortskip}{2.5pt}
%
%\vspace{-8pt}

\begin{abstract}
%	\vspace{-8pt}
%Recent work on variational objectives for deep generative models often makes the implicit assumption that tighter evidence bounds (ELBOs) are better objectives.  
\vspace{2pt}
We provide theoretical and empirical evidence that using tighter \glspl{ELBO}
can be
detrimental to the process of learning an inference network by reducing the 
signal-to-noise ratio of the gradient estimator.  Our results call into question common 
implicit assumptions that tighter \glspl{ELBO} are better variational objectives for 
simultaneous model learning and inference amortization schemes.
Based on our insights, we introduce three new algorithms:  the partially importance
weighted auto-encoder (\textsc{piwae}), the multiply
importance weighted auto-encoder (\textsc{miwae}), and the combination importance weighted
auto-encoder (\textsc{ciwae}), each of which includes the standard importance
weighted auto-encoder (\textsc{iwae}) as a special case.  We show that each
can deliver improvements over \textsc{iwae}, even when performance is measured
by the \textsc{iwae} target itself. Furthermore, our results suggest that \textsc{piwae} 
may be able to deliver simultaneous improvements in the training of both
the inference and generative networks.
%, suggesting that 
%further investigation
%is required to assess the relative utility of different approaches.  
% Based on our
%insights, we introduce a new approach for training deep generative models, the 
%partially importance-weighted auto-encoder (\textsc{piwae}), that uses different objectives for
%training the generative and inference networks.  These objectives can be simply estimated
%using a common set of samples and, despite only requiring minimal algorithmic changes
%to previous approaches, can provide substantial improvements to the learning process.
%\vspace{-8pt}
\end{abstract}

%\vspace{-15pt}

\input{intro}
%\input{background}
%\input{literature}
\input{snr}
\input{experiments}
\input{algs}
\input{experiments-alg}
%\input{piwae}
\input{conclusion}

%\section{Conclusions and Future Work}


\clearpage

\section*{Acknowledgments}

TR and YWT are supported in part by the European Research Council under the European Union's Seventh Framework Programme (FP7/2007--2013) / ERC grant agreement no. 617071. 
TAL is supported by a Google studentship, project code DF6700.
MI is supported by the UK EPSRC CDT in Autonomous Intelligent Machines
and Systems.
CJM is funded by a DeepMind Scholarship.
FW is supported under DARPA PPAML through the U.S. AFRL
under Cooperative Agreement FA8750-14-2-0006, Sub Award number 61160290-111668.

\bibliography{refs}


\clearpage

\appendix
	\onecolumn
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\setlength{\abovedisplayshortskip}{5pt}
\setlength{\belowdisplayshortskip}{5pt}

\titlespacing\section{0pt}{8pt plus 2pt minus 2pt}{2pt plus 2pt minus 0pt}
\titlespacing\subsection{0pt}{8pt plus 2pt minus 2pt}{2pt plus 2pt minus 0pt}
\titlespacing\subsubsection{0pt}{8pt plus 2pt minus 2pt}{2pt plus 2pt minus 0pt}

\thispagestyle{empty} 
\rule{\textwidth}{1pt}
\vspace{-6pt}
\begin{center}
	\textbf{ \Large  Appendices for Tighter Variational Bounds are Not Necessarily
		Better}
\end{center}%\vspace{-6pt}
\rule{\textwidth}{1pt}

	\begin{minipage}{\textwidth}
		\centering
		\vspace{17pt}
		\textbf{Tom Rainforth \quad Adam R. Kosiorek \quad Tuan Anh Le \quad Chris J. Maddison \\
		 Maximilian Igl \quad Frank Wood \quad Yee Whye Teh}
		\vspace{6pt}
	\end{minipage}

\input{proof}
\input{opt_gauss}
\input{hv_exp}
\input{other_settings}
\input{toy-gauss}

%
%\section*{Acknowledgements}
%
%Tom Rainforth is supported by a BP industrial grant. Robert Cornish is supported by an NVIDIA scholarship. Frank Wood is supported under DARPA PPAML through the U.S. AFRL under Cooperative Agreement FA8750-14-2-0006, Sub Award number 61160290-111668.


\end{document}
