
\section{Conclusions}
We have provided theoretical and empirical evidence that algorithmic approaches to increasing the tightness of the \gls{ELBO} independently to the expressiveness of the inference network can be detrimental to learning by reducing the
signal-to-noise ratio of the inference network gradients.
Experiments on a simple latent variable model confirmed our theoretical findings.
We then exploited these insights to introduce three estimators,
\gls{PIWAE},~\gls{MIWAE}, and~\gls{CIWAE} and showed that each
can deliver improvements over~\gls{IWAE}, even when the metric
used for this assessment is the~\gls{IWAE} target itself.  In particular, 
each was able to deliver improvement in the training of the inference network,
while maintaining the quality of the learned generative network.

Whereas \gls{MIWAE} and \gls{CIWAE} mostly allow for balancing the requirements of
the inference and generative networks, \gls{PIWAE} appears to be able to offer
simultaneous improvements to both, with the improved training of the inference network
having a knock-on effect on the generative network.  Key to achieving this is, is its use
of separate targets for the two networks, opening up interesting avenues for
future work.

