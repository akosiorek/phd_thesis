% !Tex root=./main.tex

\section{Evidence Lower Bounds and the Importance Weighted Auto-Encoder}

More generally, as shown by~\cite{Maddison2017filtering,anonymous2018auto-encoding}, one can express the bound
between a general \gls{ELBO} and the log marginal likelihood $\log Z_P$ given a non-negative unbiased estimator $\hat{Z}_P$
of $Z_P$ and a proposal distribution $Q(x)$ using
\begin{align}
\ELBO &= \int Q(x) \log \hat Z_P(x) \,\mathrm dx = \log Z_P - \KL{Q}{P},
\label{eqn:aesmc/bias/elbo3} \\
\text{where} \quad P(x) &= \frac{Q(x) \hat Z_P(x)}{Z_P} \label{eqn:aesmc/bias/P}
\end{align}
is a normalized target density.  We remind the reader that $\KL{Q}{P}\ge0$ with equality holding if and only if
$P=Q$.
In the case of \glspl{IWAE}, then taking $Q$ and $\hat Z_P$ as per \eqref{eq:background/q_is_z_is} yields
\begin{align}
\ELBO_{\text{IS}}(\theta, \phi, y) &= \log p_{\theta}(y) - \KL{Q_{\text{IS}}}{P_{\text{IS}}}, \text{ where } \label{eqn:aesmc/bias/elbo_is} \\
P_{\text{IS}}(x^{1:K}) &= \frac{1}{K} \sum_{k = 1}^K \left(q(x^1 \given y) \cdots q(x^{k - 1} \given y) p(x^k \given y) q(x^{k + 1} \given y) \cdots q(x^K \given y) \right),
\end{align}
while similar representations can be straightforwardly derived such as those of~\cite{Le2017auto,Maddison2017filtering,Naesseth2017variational}.

Using these results,~\cite{anonymous2018auto-encoding} go on to show that
$Q_{\text{IS}}(x^{1:K}) = P_{\text{IS}}(x^{1:K})$ for all $x^{1:K}$ if and only if $q(x \given y) = p(x \given y)$ for all $x$.
Consequently, the global optimum of $\{\theta,\phi\}$ for the \gls{IWAE} (presuming the inference network is flexible enough to always be able to
match the generative network) is the same as that for the~\gls{VAE}.  This
suggests that the advantages of the \gls{IWAE} must
originate either from better efficiency in the training procedure or less susceptibility 
to local optima. We thus
argue that the difference in tightness between the two \glspl{ELBO} is an insufficient measure of the relative utility of the two
approaches and that one should instead look to the variance of the gradient estimates relative to their expected magnitude and
susceptibility to local optima to asses the relative merits of the different choices of \gls{ELBO}.  Though it is beyond the
scope of this work to properly assess the latter of these, investigation of the former will form the focus for the rest of the paper.